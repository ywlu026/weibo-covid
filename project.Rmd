---
title: "Project"
author: "Yiwen Lu"
date: '2022-05-29'
output: pdf_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message=FALSE}
library(readr)
library(jiebaR)
library(tidyverse)
library(stringr)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.corpora)
library(ggplot2) # For plotting word frequencies
library(tidyverse)
library(wordcloud2)
library(keyATM)
library(tidytext)
```

# Load Data
```{r}
# load data
data <- read.csv("结果文件/清零/清零.csv")
# select columns needed
newdata <- data %>%
  select(c(`发布时间`,`微博正文`,`点赞数`)) %>%
  mutate(date = substr(`发布时间`,1,10)) %>%
  select(-c(`发布时间`))
# date format
newdata$date <- as.Date(newdata$date)
# filter date for May 1
newdata <- newdata %>% filter(date=="2022-05-01")
# count character length
newdata$char <- nchar(as.character(newdata$微博正文))
sum(newdata$char) #The result is a bit larger than 500,000
# reset index
newdata$index <- 1:nrow(newdata)
```

# Compare Jieba and Quanteda segmentation
## Jieba
```{r Jieba-Pre, message=FALSE}
text <- newdata$微博正文
# define segmentation without removing anything
my_seg <- worker(bylines = T)
jieba_tokens <- segment(text,my_seg)
jieba_summary <- as.data.frame(summary(jieba_tokens))
jieba_summary %>%
  pivot_wider(names_from = Var2,values_from = Freq) %>%
  mutate(Length = as.numeric(Length)) %>%
  ggplot(aes(x=Length)) +
  geom_histogram(fill = "white", color = "lightblue",binwidth=20) +
  xlab("Tokens") + 
  ggtitle("Overview of Jieba Corpus")
```

## Quanteda
```{r Quanteda-Pre, message=FALSE}
# corpus
text_corp <- corpus(text)

# overview
text_corp_overview <- summary(text_corp, ndoc(text_corp))
text_corp_overview %>%
  ggplot(aes(Tokens)) +
  geom_histogram(fill = "white", color = "lightblue",binwidth = 20) + 
  ggtitle("Overview of Quanteda Corpus")
```

## Compare tokenized tweet
```{r}
# Compare tokens
quanteda_tokens <- tokens(text_corp)

# Compare tokenized tweet
newdata$微博正文[[1]]
quanteda_tokens[[1]]
jieba_tokens[[1]]
```

# Process JiebaR segmentation under Quanteda Framework
```{r}
# Define segmentation rules
my_seg <- worker(bylines = T,
                 # use user-defined stopwords
                 stop_word = "stopwords-zh.txt",
                 symbol = F)
```

## Key Word in Context
```{r}
# Define Chinese punctuations to remove
CHUNK_DELIMITER <- "[，。！？；：\n]+"
```


```{r}
# Create tokens object using `JiebaR`
newdata$微博正文 %>%
  map(str_split, pattern=CHUNK_DELIMITER,simplify=TRUE) %>%
  map(segment,my_seg) %>%
  map(unlist) %>%
  as.tokens() -> weibo_tokens
```


```{r}
# kwic
kwic <- as.data.frame(kwic(weibo_tokens, pattern="*清零*", window = 10))

kwic %>%
  group_by(pre) %>%
  tally() %>%
  arrange(desc(n))

kwic %>% 
  group_by(post) %>%
  tally() %>%
  arrange(desc(n))
```

## Word Frequency
```{r}
# line tokenization
weibo_line <- newdata %>%
  unnest_tokens(
    output = line,
    input = 微博正文,
    token = function (x)
      str_split(x, CHUNK_DELIMITER)
  ) %>%
  group_by(index) %>%
  mutate(line_id = row_number()) %>%
  ungroup

# word tokenization
weibo_word <- weibo_line %>%
  unnest_tokens(
    output = word,
    input = line,
    token = function(x)
      segment(x,jiebar = my_seg)
  ) %>%
  group_by(index) %>%
  mutate(word_id = row_number()) %>%
  ungroup

# overview
weibo_word %>% head(10)

# word frequency
weibo_word_freq <- weibo_word %>%
  #str_replace(pattern = "^[a-zA-Z0-9_]*$", replacement = "") %>%
  filter(word %>% str_detect(pattern = "\\D+")) %>%
  count(word) %>%
  arrange(desc(n))

# plot word cloud
weibo_word_freq %>%
  filter(n > 100) %>%
  filter(nchar(word) >= 2) %>%
  wordcloud2()
```

# Keyword-Assisted Topic Model
```{r}
# create dfm
data_dfm <- dfm(weibo_tokens)

# read data
keyATM_docs <- keyATM_read(texts = data_dfm)
summary(keyATM_docs)

# process keywords
keywords <- list(Government = c("国家","发展","政策","中国","经济"),
                 Lockdown = c("上海","出","小区","生活","封","隔离"),
                 COVID = c("病毒","新冠","新增","阳性","感染"),
                 Supplies = c("人民","物资","生活","希望","工作"))

# fit the model
out <- keyATM(docs              = keyATM_docs,    # text input
              no_keyword_topics = 4,              # number of topics without keywords
              keywords          = keywords,       # keywords
              model             = "base",         # select the model
              options           = list(seed = 250))

# keywords by topics
df_out <- top_words(out)
```
```{r}
install.packages("gridExtra")
library(gridExtra)
png("test.png", height = 50*nrow(df_out), width = 200*ncol(df_out))
grid.table(df_out)
dev.off()
```

```{r}
trans <- read_csv("trans.csv")
```
```{r}
png("trans.png", height = 5000*nrow(trans), width = 20000*ncol(trans))
grid.table(trans)
dev.off()
```

